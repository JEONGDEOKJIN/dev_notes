# AB TEST 방법론

---

## 요약

```
[목적]
"이 차이가 우연인가, 진짜인가?"를 통계적으로 판단하는 것

[핵심 흐름]
1. 가설 설정 → 2. 샘플 사이즈 계산 → 3. 테스트 실행 → 4. 통계 검증 → 5. 의사결정

[핵심 지표]
- p-value < 0.05 → 통계적으로 유의미
- p-value > 0.05 → 유의미하지 않음 (우연일 수 있음)
```

---

## KPI

| 지표 | 설명 | 기준 |
|------|------|------|
| p-value | 차이가 우연히 발생할 확률 | < 0.05 |
| 유의수준 (α) | 허용하는 1종 오류 확률 | 보통 0.05 (5%) |
| 검정력 (1-β) | 실제 차이를 발견할 확률 | 보통 0.8 (80%) |
| MDE | 감지하고 싶은 최소 효과 크기 | 비즈니스에 따라 결정 |

---

## 우선순위 (AB TEST 적용 범위)

```
[1순위] 광고 소재 AB TEST
- 플랫폼 자체 기능 활용 (메타, 구글)
- 개발 불필요

[2순위] 상세페이지 AB TEST
- 기존 툴 검토 후 개발 여부 결정
- VWO, Optimizely 등

[3순위] 결과 분석 자동화
- 스프레드시트 또는 스크립트
```

---

# AB TEST 전체 흐름

```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│  1. 가설    │ →  │  2. 샘플    │ →  │  3. 실행    │ →  │  4. 검증    │ →  │  5. 결정    │
│    설정     │    │   사이즈    │    │             │    │             │    │             │
└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘
```

---

## 1. 가설 설정

### 가설의 구조

```
[귀무가설 (H0)]
- "A안과 B안의 차이가 없다"
- 기각하고 싶은 가설

[대립가설 (H1)]
- "B안이 A안보다 성과가 좋다"
- 증명하고 싶은 가설
```

### 좋은 가설의 조건

- [ ] 구체적인 수치가 있다
- [ ] 측정 가능하다
- [ ] 기간이 명시되어 있다

### 예시

```
[나쁜 가설]
"새 소재가 더 좋을 것이다"

[좋은 가설]
"새 헤드라인(B)이 기존 헤드라인(A)보다
 CTR이 15% 이상 높을 것이다 (1주일 테스트 기준)"
```

### 가설 설정 템플릿

```
테스트명: _______________
테스트 기간: ___ ~ ___

[귀무가설 H0]
A안과 B안의 _______(지표)에 차이가 없다

[대립가설 H1]
B안의 _______(지표)가 A안보다 ___% 이상 높다

[성공 기준]
- p-value < 0.05
- 개선폭 > ___%
```

---

## 2. 샘플 사이즈 계산

### 왜 중요한가?

```
샘플 너무 적으면 → 진짜 차이 있어도 못 찾음 (Type II Error)
샘플 너무 많으면 → 리소스 낭비, 시간 낭비

⚠️ 반드시 테스트 시작 전에 계산해야 함
   (나중에 하면 cherry-picking 위험)
```

### 필요한 입력값

| 항목 | 설명 | 일반적 값 |
|------|------|----------|
| Baseline 전환율 | 현재 A안의 전환율 | 실제 데이터 기반 |
| MDE (Minimum Detectable Effect) | 감지하고 싶은 최소 개선폭 | 10~20% |
| 유의수준 (α) | 1종 오류 허용 확률 | 0.05 (5%) |
| 검정력 (1-β) | 2종 오류 방지 확률 | 0.8 (80%) |

### 계산 예시

```
[입력]
- 현재 전환율: 2%
- 기대 개선폭: 20% (2% → 2.4%)
- 유의수준: 0.05
- 검정력: 0.8

[결과]
→ 각 그룹당 필요 샘플: 약 3,900명
→ 총 필요 샘플: 약 7,800명
```

### 샘플 사이즈 계산 도구

```
[온라인 계산기]
1. Evan Miller: https://www.evanmiller.org/ab-testing/sample-size.html
2. Optimizely: https://www.optimizely.com/sample-size-calculator/
3. AB Testguide: https://abtestguide.com/calc/

[스프레드시트]
- 구글 시트 템플릿 활용 가능

[Python]
from statsmodels.stats.power import NormalIndPower
analysis = NormalIndPower()
sample_size = analysis.solve_power(effect_size=0.1, alpha=0.05, power=0.8)
```

### 샘플 사이즈 참고표

| Baseline | MDE 10% | MDE 15% | MDE 20% |
|----------|---------|---------|---------|
| 1% | 15,700 | 7,000 | 3,900 |
| 2% | 7,700 | 3,400 | 1,900 |
| 5% | 2,900 | 1,300 | 730 |
| 10% | 1,400 | 620 | 350 |

*(각 그룹당 필요 샘플 수, α=0.05, power=0.8 기준)*

---

## 3. 테스트 실행

### 실행 전 체크리스트

- [ ] 가설이 명확한가?
- [ ] 샘플 사이즈를 계산했는가?
- [ ] 테스트 기간을 정했는가?
- [ ] A/B 그룹 배분 방법을 정했는가?
- [ ] 성공 지표(KPI)를 정했는가?

### 실행 원칙

```
[필수 원칙]

1. 무작위 배정 (Randomization)
   - A/B 그룹을 무작위로 나눔
   - 선택 편향 제거

2. 동시 실행 (Simultaneity)
   - A와 B를 같은 시간대에 실행
   - 시간대 효과 제거

3. 단일 변수 (Single Variable)
   - 한 번에 하나의 변수만 테스트
   - 여러 변수 바꾸면 뭐가 효과인지 모름

4. 변수 고정 (Control)
   - 테스트 기간 중 다른 조건 변경 금지

5. 중간 종료 금지 (No Peeking)
   - 사전에 정한 샘플까지 기다림
```

### Peeking Problem (중요!)

```
❌ 잘못된 방식:
Day 1: "어 B가 좋아 보이는데?"
Day 3: "아직 B가 좋네, 테스트 종료!"
→ p-value가 왜곡됨 (실제보다 낮게 나옴)

✅ 올바른 방식:
"샘플 3,900명 도달할 때까지 기다린다"
→ 중간에 결과 보더라도 의사결정하지 않음

[대안: Sequential Testing]
- 중간 확인이 꼭 필요하면 Sequential Testing 방법 사용
- 유의수준을 보정해서 적용
```

### 테스트 실행 기록 템플릿

```
[테스트 기본 정보]
테스트명:
시작일:
종료 예정일:
담당자:

[그룹 설정]
A안 (Control):
B안 (Variant):

[트래픽 배분]
A: __% / B: __%

[성공 지표]
Primary:
Secondary:

[필요 샘플]
각 그룹: ___명
총: ___명

[현재 진행 상황]
날짜 | A 노출 | A 전환 | B 노출 | B 전환 | 비고
-----|-------|-------|-------|-------|-----
     |       |       |       |       |
```

---

## 4. 통계 검증

### 핵심 개념: p-value

```
[정의]
"A와 B가 실제로 같다고 가정했을 때,
 관찰된 차이(또는 더 큰 차이)가 우연히 발생할 확률"

[해석]
p = 0.03 → "이 차이가 우연일 확률이 3%"
         → 3%는 낮으니까, 우연이 아닐 가능성 높음
         → 통계적으로 유의미!

p = 0.15 → "이 차이가 우연일 확률이 15%"
         → 15%는 꽤 높음, 우연일 수 있음
         → 유의미하다고 말하기 어려움
```

### 판단 기준

```
p-value < 0.05  →  통계적으로 유의미 (Significant)
p-value ≥ 0.05  →  유의미하지 않음 (Not Significant)

⚠️ 주의:
"유의미하지 않음" ≠ "효과가 없음"
"유의미하지 않음" = "효과가 있다고 말할 충분한 증거가 없음"
```

### 검정 방법 선택

| 비교 대상 | 데이터 유형 | 사용할 검정 |
|----------|-----------|------------|
| A vs B 전환율 | 비율 (%) | **카이제곱 검정** 또는 **Z-검정** |
| A vs B 평균값 | 연속형 (금액, 시간 등) | **t-검정** |
| A vs B vs C | 3개 이상 그룹 | **ANOVA** + 사후검정 |

### 실제 계산 방법

#### 방법 1: 온라인 계산기

```
[추천 도구]
1. AB Test Calculator: https://www.evanmiller.org/ab-testing/chi-squared.html
2. Neil Patel: https://neilpatel.com/ab-testing-calculator/

[입력값]
- A 방문자 수: 10,000
- A 전환 수: 200
- B 방문자 수: 10,000
- B 전환 수: 250

[결과]
- p-value: 0.0067
- 유의미함!
```

#### 방법 2: 스프레드시트 (Google Sheets / Excel)

```
[카이제곱 검정]

데이터 배치:
        전환    비전환
A안     200     9800
B안     250     9750

수식:
=CHISQ.TEST(A1:B2, 기대값범위)

또는 직접 계산:
=CHITEST(실제값, 기대값)
```

#### 방법 3: Python

```python
from scipy import stats

# 데이터
a_conversions, a_total = 200, 10000  # A안: 2.0%
b_conversions, b_total = 250, 10000  # B안: 2.5%

# 방법 1: 카이제곱 검정
contingency = [[a_conversions, a_total - a_conversions],
               [b_conversions, b_total - b_conversions]]
chi2, p_value, dof, expected = stats.chi2_contingency(contingency)
print(f"카이제곱 p-value: {p_value:.4f}")

# 방법 2: 비율 Z-검정
from statsmodels.stats.proportion import proportions_ztest
count = [a_conversions, b_conversions]
nobs = [a_total, b_total]
z_stat, p_value = proportions_ztest(count, nobs, alternative='two-sided')
print(f"Z-검정 p-value: {p_value:.4f}")
```

#### 방법 4: 간이 계산 (수기)

```
[전환율 차이의 표준오차]

SE = √(p*(1-p)*(1/n1 + 1/n2))

여기서 p = 합동 전환율 = (전환1 + 전환2) / (n1 + n2)

[Z-score]
Z = (p1 - p2) / SE

[p-value]
|Z| > 1.96 이면 p < 0.05 (유의미)
|Z| > 2.58 이면 p < 0.01 (매우 유의미)
```

### 신뢰구간 (Confidence Interval)

```
[정의]
"실제 효과가 이 범위 안에 있을 확률이 95%"

[예시]
B안의 전환율 상승: 0.3% ~ 0.7% (95% CI)

[해석]
- B안이 A안보다 0.3%~0.7% 높다고 95% 확신
- 신뢰구간이 0을 포함하지 않으면 유의미
```

---

## 5. 의사결정

### 결과 해석 프레임워크

```
┌─────────────────────────────────────────────────────┐
│                  p-value < 0.05?                    │
└─────────────────────────────────────────────────────┘
              │                    │
             YES                   NO
              │                    │
              ▼                    ▼
┌─────────────────────┐  ┌─────────────────────┐
│  통계적으로 유의미   │  │ 유의미하지 않음      │
│                     │  │                     │
│  다음 질문:         │  │  옵션:              │
│  실질적으로도       │  │  1. 샘플 더 모으기   │
│  의미있는 차이인가? │  │  2. 다른 변수 테스트 │
└─────────────────────┘  │  3. 현상 유지       │
              │          └─────────────────────┘
              ▼
┌─────────────────────┐
│ 비즈니스 영향도     │
│ 계산               │
│                    │
│ 월 전환 1000건     │
│ × 0.5%p 개선       │
│ = 월 5건 추가      │
│ × 객단가 100만원   │
│ = 월 500만원 증가  │
└─────────────────────┘
```

### 의사결정 체크리스트

```
□ p-value가 0.05 미만인가?
□ 신뢰구간이 0을 포함하지 않는가?
□ 실질적으로 의미있는 개선폭인가?
□ 다른 지표에 부정적 영향은 없는가?
□ 구현 비용 대비 효과가 있는가?
```

### 결과 기록 템플릿

```
[테스트 결과 요약]

테스트명:
테스트 기간:
총 샘플 수:

[결과 데이터]
        노출      전환      전환율
A안     10,000    200       2.00%
B안     10,000    250       2.50%
차이    -         +50       +0.50%p (+25%)

[통계 검증]
p-value: 0.0067
95% CI: [0.15%, 0.85%]
결론: 통계적으로 유의미함

[비즈니스 영향]
- 월간 예상 추가 전환: ___건
- 월간 예상 추가 매출: ___원

[의사결정]
□ B안 채택
□ 추가 테스트 필요
□ 현상 유지

[다음 액션]
-
```

---

# 실무 적용 예시

## 예시 1: 메타 광고 소재 AB TEST

```
[1. 가설]
"후기 영상 소재(B)가 이미지 소재(A)보다 CTR이 20% 이상 높을 것"

[2. 샘플 계산]
- 현재 CTR: 1.2%
- 기대 개선: 20% (1.2% → 1.44%)
- 필요 샘플: 각 그룹 약 40,000 노출

[3. 실행]
- 메타 광고 관리자 → A/B 테스트 기능 사용
- 예산: 50:50 배분
- 기간: 7일

[4. 결과]
A안 (이미지): 42,000 노출, 504 클릭 (CTR 1.20%)
B안 (영상):   41,500 노출, 622 클릭 (CTR 1.50%)

p-value: 0.0012 → 유의미!

[5. 결론]
B안(영상 소재) 채택
다음 테스트: 영상 길이별 테스트 (15초 vs 30초)
```

## 예시 2: 랜딩페이지 헤드라인 AB TEST

```
[1. 가설]
"숫자 강조 헤드라인(B)이 일반 헤드라인(A)보다
 상담신청 전환율이 15% 이상 높을 것"

A: "게임 개발자로 커리어 전환하세요"
B: "6개월 만에 게임 개발자로 취업, 수강생 89% 달성"

[2. 샘플 계산]
- 현재 전환율: 3%
- 기대 개선: 15% (3% → 3.45%)
- 필요 샘플: 각 그룹 약 5,200명

[3. 실행]
- VWO 또는 직접 구현
- 트래픽 50:50 배분
- 기간: 2주

[4. 결과]
A안: 5,500명 방문, 165명 전환 (3.00%)
B안: 5,400명 방문, 189명 전환 (3.50%)

p-value: 0.043 → 유의미!

[5. 결론]
B안 채택 (숫자 강조 헤드라인)
```

---

# 자주 하는 실수

## 1. 샘플 부족

```
❌ "3일 돌렸는데 B가 좋아 보여서 적용했어요"
✅ 사전에 계산한 샘플 사이즈까지 기다림
```

## 2. 중간에 엿보기 (Peeking)

```
❌ 매일 결과 확인하고 좋아 보이면 종료
✅ 종료 조건(샘플 or 기간) 도달 전까지 의사결정 보류
```

## 3. 여러 변수 동시 변경

```
❌ 헤드라인 + 이미지 + 버튼 색상 다 바꿈
✅ 한 번에 하나씩 테스트
```

## 4. p-value만 보기

```
❌ "p < 0.05니까 무조건 적용!"
✅ 실질적 개선폭 + 비즈니스 영향도 함께 고려
```

## 5. 패배 선언 너무 빠름

```
❌ "유의미하지 않으니 B안은 효과 없음"
✅ "효과가 있다고 말할 증거가 부족함" (샘플 더 필요할 수 있음)
```

---

# 도구 모음

## 샘플 사이즈 계산

| 도구 | URL |
|------|-----|
| Evan Miller | https://www.evanmiller.org/ab-testing/sample-size.html |
| Optimizely | https://www.optimizely.com/sample-size-calculator/ |
| AB Testguide | https://abtestguide.com/calc/ |

## 통계 검증

| 도구 | URL |
|------|-----|
| Evan Miller (카이제곱) | https://www.evanmiller.org/ab-testing/chi-squared.html |
| Neil Patel | https://neilpatel.com/ab-testing-calculator/ |
| AB Test Calculator | https://abtestcalculator.com/ |

## AB TEST 플랫폼

| 플랫폼 | 용도 | 비용 |
|--------|------|------|
| 메타 광고 관리자 | 광고 소재 테스트 | 무료 (광고비만) |
| 구글 광고 | 광고 소재 테스트 | 무료 (광고비만) |
| VWO | 웹페이지 테스트 | 유료 |
| Optimizely | 웹페이지 테스트 | 유료 |
| Google Optimize | 웹페이지 테스트 | 서비스 종료 |

---

# 다음 단계

```
[이 문서 활용]
1. 실제 AB TEST 진행 시 템플릿 활용
2. 결과 기록 및 학습 축적

[연결 Task]
→ 광고 소재 AB TEST 실행 (메타/구글 플랫폼 활용)
→ 상세페이지 AB TEST 환경 구축 (Phase 2)
→ 결과 분석 자동화 (Phase 2)
```
